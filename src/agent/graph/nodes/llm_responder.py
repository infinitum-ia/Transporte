# LLM responder node - call OpenAI with optimized prompt
import json
import logging
import os
from typing import Dict, Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from src.infrastructure.logging import get_logger
from src.agent.graph.nodes.context_builder import context_builder as build_context_prompt
from src.infrastructure.config.settings import settings

logger = logging.getLogger(__name__)


def _truncate_preview(value: str | None, limit: int) -> str:
    """Return a single-line, truncated version of the prompt or message."""
    if not value:
        return ""
    cleaned = value.replace("\n", " ").strip()
    if len(cleaned) <= limit:
        return cleaned
    return f"{cleaned[:limit-3]}..."


def llm_responder(state: Dict[str, Any]) -> Dict[str, Any]:
    """Call LLM to generate response using optimized prompt"""

    print(f"\n{'‚îÅ'*80}")
    print(f"ü§ñ [NODO 2/3] LLM RESPONDER - Generando Respuesta con OpenAI")
    print(f"{'‚îÅ'*80}")

    # Get system prompt from context_builder
    system_prompt = state.get("llm_system_prompt", "")
    if not system_prompt:
        logger.warning("No system prompt in state, re-running context_builder")
        state = build_context_prompt(state)
        system_prompt = state.get("llm_system_prompt", "")
    if not system_prompt:
        logger.warning("No system prompt after rebuild, using fallback")
        state["agent_response"] = "Disculpe, hubo un problema t√©cnico. ¬øPuede repetir, por favor?"
        state["next_phase"] = state.get("current_phase", "GREETING")
        return state

    # Get conversation history (last few messages)
    messages = state.get("messages", [])
    last_user_message = None
    for msg in reversed(messages):
        if isinstance(msg, dict):
            if msg.get("role") == "user":
                last_user_message = msg.get("content", "")
                break
        elif hasattr(msg, "type") and msg.type == "human":
            last_user_message = msg.content
            break

    if not last_user_message:
        logger.warning("No user message found in state")
        state["agent_response"] = "Disculpe, no entend√≠. ¬øMe puede repetir?"
        state["next_phase"] = state.get("current_phase", "GREETING")
        return state

    try:
        # Initialize LLM (force JSON output when model supports it)
        llm_kwargs = {
            "openai_api_key": settings.OPENAI_API_KEY,
            "model": settings.OPENAI_MODEL,
            "temperature": settings.OPENAI_TEMPERATURE,
            "max_tokens": settings.OPENAI_MAX_TOKENS,
        }
        model_name = (settings.OPENAI_MODEL or "").lower()
        if "gpt-4o" in model_name or "gpt-4" in model_name:
            llm_kwargs["response_format"] = {"type": "json_object"}

        llm = ChatOpenAI(**llm_kwargs)

        # Build messages for LLM with full conversation history
        llm_messages = [SystemMessage(content=system_prompt)]

        # Add conversation history (all messages)
        for msg in messages:
            if isinstance(msg, dict):
                # Dict format (from Redis)
                role = msg.get("role")
                content = msg.get("content", "")
                if role == "user":
                    llm_messages.append(HumanMessage(content=content))
                elif role == "assistant":
                    from langchain_core.messages import AIMessage
                    llm_messages.append(AIMessage(content=content))
            elif hasattr(msg, "type"):
                # LangChain message format
                llm_messages.append(msg)

        policy_ids = [
            violation.get("policy_id") or violation.get("policy_name") or "desconocida"
            for violation in state.get("policy_violations", [])
            if isinstance(violation, dict)
        ]
        logger_preview = get_logger()
        logger_preview.log_langgraph_state(
            session_id=state.get("session_id", "unknown"),
            current_phase=str(state.get("current_phase", "N/A")),
            call_direction=str(state.get("call_direction", "N/A")),
            agent_name=state.get("agent_name"),
            patient_name=state.get("patient_full_name") or state.get("caller_name"),
            prompt_preview=_truncate_preview(system_prompt, 280),
            user_message_preview=_truncate_preview(last_user_message, 200),
            policy_ids=policy_ids,
            escalation_required=bool(state.get("escalation_required")),
            message_count=len(messages)
        )

        # Call LLM
        print(f"\nüß† [AGENT B] OpenAI GPT ({settings.OPENAI_MODEL}) - Generando respuesta...")
        print(f"   ‚û§ Prompt: {len(system_prompt)} caracteres (~{len(system_prompt.split())} palabras)")
        print(f"   ‚û§ Historial: {len(messages)} mensajes")
        print(f"   ‚û§ Temperatura: {settings.OPENAI_TEMPERATURE}")
        print(f"   ‚û§ Max tokens: {settings.OPENAI_MAX_TOKENS}")
        logger.info(f"Calling LLM for phase: {state.get('current_phase')}")

        # ALWAYS print prompts for debugging (truncated in console, full in file)
        print(f"\n{'‚îÄ'*80}")
        print("üìÑ PROMPT PREVIEW (primeras 500 caracteres):")
        print(f"{'‚îÄ'*80}")
        print(system_prompt)
        print(f"\n{'‚îÄ'*80}")
        print(f"üí¨ HISTORIAL DE CONVERSACI√ìN ({len(messages)} mensajes):")
        for i, msg in enumerate(messages[-5:], 1):  # Show last 5 messages
            if isinstance(msg, dict):
                role = msg.get("role", "unknown")
                content = msg.get("content", "")[:100]
            elif hasattr(msg, "type"):
                role = msg.type
                content = msg.content[:100] if hasattr(msg, "content") else ""
            else:
                role = "unknown"
                content = str(msg)[:100]
            print(f"   {i}. [{role}]: {content}...")
        print(f"{'‚îÄ'*80}\n")

        # Guardar prompt completo a archivo para inspecci√≥n
        try:
            import datetime
            timestamp = datetime.datetime.now().strftime("%H%M%S")
            prompt_file = f"C:\\Users\\Administrador\\Documents\\Transporte\\prompt_debug_{timestamp}.txt"
            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write("="*100 + "\n")
                f.write("PROMPT SYSTEM COMPLETO\n")
                f.write("="*100 + "\n\n")
                f.write(system_prompt)
                f.write("\n\n" + "="*100 + "\n")
                f.write(f"HISTORIAL DE CONVERSACI√ìN ({len(messages)} mensajes)\n")
                f.write("="*100 + "\n\n")
                for i, msg in enumerate(messages, 1):
                    if isinstance(msg, dict):
                        role = msg.get("role", "unknown")
                        content = msg.get("content", "")
                    elif hasattr(msg, "type"):
                        role = msg.type
                        content = msg.content if hasattr(msg, "content") else str(msg)
                    else:
                        role = "unknown"
                        content = str(msg)
                    f.write(f"Mensaje {i} [{role}]:\n{content}\n\n")
                f.write("="*100 + "\n")
                f.write("STATE INFORMACI√ìN\n")
                f.write("="*100 + "\n\n")
                f.write(f"Session ID: {state.get('session_id', 'unknown')}\n")
                f.write(f"Fase: {state.get('current_phase', 'N/A')}\n")
                f.write(f"Turno: {state.get('turn_count', 0)}\n")
                f.write(f"Contact name en state: {state.get('contact_name', 'N/A')}\n")
                f.write(f"Contact relationship en state: {state.get('contact_relationship', 'N/A')}\n")
                f.write(f"Contact age en state: {state.get('contact_age', 'N/A')}\n")
            print(f"üíæ Prompt guardado en: {prompt_file}")
        except Exception as e:
            logger.warning(f"No se pudo guardar prompt debug: {e}")

        print(f"‚è≥ Esperando respuesta del LLM...")
        response = llm.invoke(llm_messages)
        llm_output = response.content
        state["_llm_raw_output"] = llm_output
        print(f"‚úÖ Respuesta recibida del LLM\n")

        # Parse JSON response
        try:
            parsed = json.loads(llm_output)
            state["agent_response"] = parsed.get("agent_response", "")
            state["next_phase"] = parsed.get("next_phase", state.get("current_phase", "GREETING"))
            state["requires_escalation"] = parsed.get("requires_escalation", False)
            state["extracted_data"] = parsed.get("extracted", {})

            # NEW: Validate response with rules (no LLM cost)
            validation_attempts = state.get("validation_attempt_count", 0)
            validation_result = _validate_response_rules(state["agent_response"], state)

            if validation_result['has_critical_error'] and validation_attempts < 2:
                # Critical error detected, regenerate ONCE
                print(f"‚ö†Ô∏è  [VALIDACI√ìN] Error detectado: {validation_result['error']}")
                print(f"   üîÑ Regenerando respuesta (intento {validation_attempts + 1}/2)...")

                state["validation_attempt_count"] = validation_attempts + 1

                # Add correction to prompt
                correction_prompt = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë CORRECCI√ìN DE VALIDACI√ìN                                      ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Tu respuesta anterior ten√≠a un ERROR CR√çTICO:
{validation_result['error']}

CORRIGE este problema y genera una nueva respuesta.
"""
                # Rebuild messages with correction and full history
                corrected_system_prompt = system_prompt + correction_prompt
                llm_messages_corrected = [SystemMessage(content=corrected_system_prompt)]

                # Add full conversation history
                for msg in messages:
                    if isinstance(msg, dict):
                        role = msg.get("role")
                        content = msg.get("content", "")
                        if role == "user":
                            llm_messages_corrected.append(HumanMessage(content=content))
                        elif role == "assistant":
                            from langchain_core.messages import AIMessage
                            llm_messages_corrected.append(AIMessage(content=content))
                    elif hasattr(msg, "type"):
                        llm_messages_corrected.append(msg)

                # Regenerate
                print(f"‚è≥ Regenerando con correcci√≥n...")
                response = llm.invoke(llm_messages_corrected)
                llm_output = response.content
                state["_llm_raw_output"] = llm_output

                # Re-parse
                try:
                    parsed = json.loads(llm_output)
                    state["agent_response"] = parsed.get("agent_response", "")
                    state["next_phase"] = parsed.get("next_phase", state.get("current_phase"))
                    state["requires_escalation"] = parsed.get("requires_escalation", False)
                    state["extracted_data"] = parsed.get("extracted", {})

                    # Validate again (but don't loop again)
                    validation_result2 = _validate_response_rules(state["agent_response"], state)
                    if validation_result2['has_critical_error']:
                        print(f"‚ö†Ô∏è  A√∫n tiene errores, pero se alcanz√≥ l√≠mite de intentos")
                    else:
                        print(f"‚úÖ Respuesta corregida exitosamente")
                except json.JSONDecodeError:
                    pass  # Use as-is if parsing fails

            elif validation_result['has_critical_error']:
                # Reached max attempts, log but continue
                print(f"‚ö†Ô∏è  [VALIDACI√ìN] Error detectado pero l√≠mite alcanzado (2/2 intentos)")
                logger.warning(f"Validation errors (max attempts): {validation_result['errors']}")

            else:
                # No errors, validation passed
                print(f"‚úÖ [VALIDACI√ìN] Respuesta aprobada")

            print(f"‚úÖ [NODO 2/3] LLM completado exitosamente")
            print(f"   üí¨ Respuesta: '{state['agent_response'][:80]}...'")
            print(f"   üîÑ Pr√≥xima fase: {state.get('current_phase')} ‚Üí {state['next_phase']}")
            extracted_count = len([v for v in state['extracted_data'].values() if v])
            print(f"   üìä Datos extra√≠dos: {extracted_count} campos")
            for key, value in state['extracted_data'].items():
                if value:
                    print(f"      ‚Üí {key}: {value}")
            if state['requires_escalation']:
                print(f"   ‚ö†Ô∏è  Requiere escalaci√≥n: S√ç")
            print(f"{'‚îÅ'*80}\n")

            # Log successful LLM response
            logger_preview.log_llm_response(
                session_id=state.get("session_id", "unknown"),
                current_phase=state.get("current_phase", "N/A"),
                agent_response=state["agent_response"],
                next_phase=state["next_phase"],
                extracted_data=state["extracted_data"],
                requires_escalation=state["requires_escalation"]
            )
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response: {llm_output}")
            logger_preview.log_llm_error(
                session_id=state.get("session_id", "unknown"),
                error_type="JSONDecodeError",
                error_message=f"Failed to parse: {str(e)}",
                current_phase=state.get("current_phase", "N/A")
            )
            # If LLM didn't return JSON, try to use it as response anyway
            state["agent_response"] = llm_output
            state["next_phase"] = state.get("current_phase", "GREETING")

    except Exception as e:
        logger.error(f"LLM call failed: {str(e)}")
        state["agent_response"] = "Disculpe, hubo un problema t√©cnico. ¬øPuede intentar m√°s tarde?"
        state["next_phase"] = state.get("current_phase", "GREETING")

    return state


def _validate_response_rules(response: str, state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate LLM response using RULES (not LLM) to detect critical errors.

    This lightweight validation catches common issues without additional LLM cost:
    1. Logical failures (wrong dates/times)
    2. Security issues (revealing data to minors)
    3. Accessibility issues (overly complex language)
    4. Consistency issues (closing without summary)

    Args:
        response: Agent response text to validate
        state: Current conversation state

    Returns:
        Dict with: {
            "has_critical_error": bool,
            "errors": list of error strings,
            "error": first error (for correction prompt) or None
        }
    """
    import re
    from datetime import datetime

    errors = []

    # 1. LOGICAL FAILURE: Incorrect dates mentioned
    appointment_date = state.get('appointment_date')
    if appointment_date:
        # Extract dates mentioned in response (simple regex for common formats)
        # Patterns: "15 de enero", "20/01", "01-20", etc.
        date_patterns = [
            r'\b(\d{1,2})\s+de\s+(\w+)',  # "15 de enero"
            r'\b(\d{1,2})/(\d{1,2})',      # "15/01"
            r'\b(\d{1,2})-(\d{1,2})'       # "15-01"
        ]

        mentioned_dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            mentioned_dates.extend(matches)

        # Simple check: if appointment_date contains a day number,
        # verify it's mentioned in the response
        if mentioned_dates:
            # Extract day from appointment_date
            try:
                if '-' in appointment_date:
                    # YYYY-MM-DD format
                    date_obj = datetime.strptime(appointment_date, '%Y-%m-%d')
                elif '/' in appointment_date:
                    # DD/MM/YYYY format
                    date_obj = datetime.strptime(appointment_date, '%d/%m/%Y')
                else:
                    date_obj = None

                if date_obj:
                    correct_day = date_obj.day
                    # Check if correct_day is in any mentioned date
                    found_correct_date = False
                    for match in mentioned_dates:
                        if str(correct_day) in str(match[0]) if isinstance(match, tuple) else str(match):
                            found_correct_date = True
                            break

                    if not found_correct_date and mentioned_dates:
                        errors.append(f"Fecha mencionada no coincide con appointment_date={appointment_date}")

            except Exception:
                pass  # Date parsing failed, skip validation

    # 2. SECURITY: Revealing data to minors
    contact_age = state.get('contact_age')
    if contact_age:
        try:
            age_int = int(contact_age)
            if age_int < 18:
                # Check if response contains sensitive data
                sensitive_keywords = ['documento', 'direcci√≥n', 'cita', 'servicio', 'fecha', 'hora']
                if any(keyword in response.lower() for keyword in sensitive_keywords):
                    errors.append("Revelando datos sensibles a menor de edad (age<18)")
        except (ValueError, TypeError):
            pass

    # 3. ACCESSIBILITY: Sentences too long (>35 words)
    sentences = [s.strip() for s in response.split('.') if s.strip()]
    long_sentences = [s for s in sentences if len(s.split()) > 35]
    if len(long_sentences) > 2:  # Allow up to 2 long sentences
        errors.append(f"Lenguaje demasiado complejo ({len(long_sentences)} frases >35 palabras)")

    # 4. CONSISTENCY: Closing without summary
    next_phase = state.get('next_phase', '')
    if next_phase in ['END', 'OUTBOUND_CLOSING']:
        # Check if response includes confirmation/summary keywords
        summary_keywords = ['confirmar', 'queda registrado', 'resumen', 'para confirmar', 'entonces']
        has_summary = any(keyword in response.lower() for keyword in summary_keywords)

        # Also check if it includes critical data (date, time, service)
        has_date_ref = any(word in response.lower() for word in ['fecha', 'd√≠a', 'enero', 'febrero', 'marzo', 'lunes', 'martes'])
        has_time_ref = any(word in response.lower() for word in ['hora', ':'])
        has_service_ref = any(word in response.lower() for word in ['terapia', 'di√°lisis', 'cita', 'servicio'])

        # If closing, should have either summary keyword OR all three references
        if not (has_summary or (has_date_ref and has_time_ref and has_service_ref)):
            errors.append("Despedida sin resumen de confirmaci√≥n")

    return {
        'has_critical_error': len(errors) > 0,
        'errors': errors,
        'error': errors[0] if errors else None
    }
