"""
Test de verificaci√≥n de la integraci√≥n emocional (Opci√≥n A).

Este script verifica que:
1. El state tiene los campos emocionales
2. El context_builder puede hacer an√°lisis unificado
3. El prompt_builder inyecta contexto emocional
4. La validaci√≥n por reglas funciona
"""

import sys
import os

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.agent.graph.state_adapters import create_initial_state
from src.agent.context_builder import ContextBuilderAgent
from src.agent.prompts.prompt_builder import build_prompt
from src.domain.value_objects.conversation_phase import ConversationPhase


def test_state_has_emotional_fields():
    """Test 1: Verificar que el state inicial tiene campos emocionales"""
    print("\n" + "="*80)
    print("TEST 1: State tiene campos emocionales")
    print("="*80)

    state = create_initial_state(
        session_id="test-123",
        call_direction="OUTBOUND",
        agent_name="Mar√≠a"
    )

    # Check emotional fields exist
    assert "emotional_memory" in state, "‚ùå Falta campo emotional_memory"
    assert "current_sentiment" in state, "‚ùå Falta campo current_sentiment"
    assert "current_conflict_level" in state, "‚ùå Falta campo current_conflict_level"
    assert "personality_mode" in state, "‚ùå Falta campo personality_mode"
    assert "emotional_validation_required" in state, "‚ùå Falta campo emotional_validation_required"
    assert "validation_attempt_count" in state, "‚ùå Falta campo validation_attempt_count"

    # Check default values
    assert state["emotional_memory"] == [], "‚ùå emotional_memory debe iniciar vac√≠a"
    assert state["current_sentiment"] == "Neutro", f"‚ùå current_sentiment debe ser 'Neutro', no '{state['current_sentiment']}'"
    assert state["current_conflict_level"] == "Bajo", f"‚ùå current_conflict_level debe ser 'Bajo', no '{state['current_conflict_level']}'"
    assert state["personality_mode"] == "Balanceado", f"‚ùå personality_mode debe ser 'Balanceado', no '{state['personality_mode']}'"
    assert state["emotional_validation_required"] == False, "‚ùå emotional_validation_required debe ser False"
    assert state["validation_attempt_count"] == 0, "‚ùå validation_attempt_count debe ser 0"

    print("‚úÖ PASS: State tiene todos los campos emocionales con valores correctos")
    return True


def test_context_builder_has_unified_analysis():
    """Test 2: Verificar que ContextBuilderAgent tiene el m√©todo unificado"""
    print("\n" + "="*80)
    print("TEST 2: ContextBuilderAgent tiene an√°lisis unificado")
    print("="*80)

    try:
        context_builder = ContextBuilderAgent()

        # Check method exists
        assert hasattr(context_builder, '_unified_context_analysis_llm'), \
            "‚ùå Falta m√©todo _unified_context_analysis_llm"

        assert hasattr(context_builder, 'build_context'), \
            "‚ùå Falta m√©todo build_context"

        print("‚úÖ PASS: ContextBuilderAgent tiene el m√©todo _unified_context_analysis_llm")
        return True

    except Exception as e:
        print(f"‚ùå FAIL: Error al inicializar ContextBuilderAgent: {e}")
        return False


def test_prompt_builder_accepts_emotional_analysis():
    """Test 3: Verificar que build_prompt acepta analisis_emocional"""
    print("\n" + "="*80)
    print("TEST 3: prompt_builder acepta analisis_emocional")
    print("="*80)

    try:
        # Test without emotional analysis
        prompt1 = build_prompt(
            phase=ConversationPhase.OUTBOUND_GREETING,
            agent_name="Mar√≠a",
            company_name="Transpormax",
            eps_name="Cosalud",
            known_data={}
        )

        assert len(prompt1) > 0, "‚ùå Prompt vac√≠o sin an√°lisis emocional"
        print(f"   ‚úì Prompt sin an√°lisis emocional: {len(prompt1)} caracteres")

        # Test WITH emotional analysis
        analisis_emocional = {
            "sentiment": "Frustraci√≥n",
            "conflict_level": "Alto",
            "personality_mode": "Balanceado",
            "emotional_validation_required": True
        }

        prompt2 = build_prompt(
            phase=ConversationPhase.OUTBOUND_SERVICE_CONFIRMATION,
            agent_name="Mar√≠a",
            company_name="Transpormax",
            eps_name="Cosalud",
            known_data={},
            analisis_emocional=analisis_emocional
        )

        assert len(prompt2) > 0, "‚ùå Prompt vac√≠o con an√°lisis emocional"
        assert len(prompt2) > len(prompt1), "‚ùå Prompt con an√°lisis deber√≠a ser m√°s largo"
        assert "USUARIO FRUSTRADO" in prompt2, "‚ùå Falta adaptaci√≥n de frustraci√≥n en prompt"
        assert "CONFLICTO ALTO" in prompt2, "‚ùå Falta menci√≥n de conflicto alto en prompt"
        assert "VALIDACI√ìN EMOCIONAL REQUERIDA" in prompt2, "‚ùå Falta menci√≥n de validaci√≥n emocional"

        print(f"   ‚úì Prompt con an√°lisis emocional: {len(prompt2)} caracteres")
        print(f"   ‚úì Contiene adaptaci√≥n de frustraci√≥n: ‚úÖ")
        print(f"   ‚úì Contiene conflicto alto: ‚úÖ")
        print(f"   ‚úì Contiene validaci√≥n emocional: ‚úÖ")
        print("‚úÖ PASS: prompt_builder correctamente inyecta contexto emocional")
        return True

    except Exception as e:
        print(f"‚ùå FAIL: Error en build_prompt: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_validation_rules_exist():
    """Test 4: Verificar que existe la funci√≥n de validaci√≥n por reglas"""
    print("\n" + "="*80)
    print("TEST 4: Validaci√≥n por reglas existe")
    print("="*80)

    try:
        from src.agent.graph.nodes.llm_responder import _validate_response_rules

        # Test con respuesta v√°lida
        state = {
            "appointment_date": "2024-01-20",
            "contact_age": "25",
            "next_phase": "OUTBOUND_SERVICE_CONFIRMATION"
        }

        response_valid = "Su cita es el 20 de enero a las 10:00 AM. ¬øConfirma?"
        result1 = _validate_response_rules(response_valid, state)

        assert "has_critical_error" in result1, "‚ùå Falta campo has_critical_error"
        assert "errors" in result1, "‚ùå Falta campo errors"
        assert "error" in result1, "‚ùå Falta campo error"

        print(f"   ‚úì Validaci√≥n de respuesta v√°lida:")
        print(f"      - has_critical_error: {result1['has_critical_error']}")
        print(f"      - errors: {result1['errors']}")

        # Test con respuesta problem√°tica (menor de edad)
        state_minor = {
            "appointment_date": "2024-01-20",
            "contact_age": "15",  # MENOR DE EDAD
            "next_phase": "OUTBOUND_SERVICE_CONFIRMATION"
        }

        response_with_data = "Su cita es el 20 de enero con documento CC 12345678 en la direcci√≥n Calle 123."
        result2 = _validate_response_rules(response_with_data, state_minor)

        assert result2['has_critical_error'] == True, "‚ùå Deber√≠a detectar revelaci√≥n a menor"
        assert len(result2['errors']) > 0, "‚ùå Deber√≠a tener errores detectados"

        print(f"   ‚úì Validaci√≥n de respuesta a menor (deber√≠a fallar):")
        print(f"      - has_critical_error: {result2['has_critical_error']} ‚úÖ")
        print(f"      - errors: {result2['errors']}")

        print("‚úÖ PASS: Validaci√≥n por reglas funciona correctamente")
        return True

    except ImportError:
        print("‚ùå FAIL: No se puede importar _validate_response_rules")
        return False
    except Exception as e:
        print(f"‚ùå FAIL: Error en validaci√≥n: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_all_tests():
    """Ejecutar todos los tests"""
    print("\n" + "#"*80)
    print("# TEST DE INTEGRACI√ìN: Opci√≥n A (Integraci√≥n Ligera)")
    print("#"*80)

    tests = [
        ("State emocional", test_state_has_emotional_fields),
        ("ContextBuilder unificado", test_context_builder_has_unified_analysis),
        ("PromptBuilder emocional", test_prompt_builder_accepts_emotional_analysis),
        ("Validaci√≥n por reglas", test_validation_rules_exist)
    ]

    results = []
    for name, test_func in tests:
        try:
            success = test_func()
            results.append((name, success))
        except Exception as e:
            print(f"\n‚ùå FAIL: {name} - Exception: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))

    # Summary
    print("\n" + "="*80)
    print("RESUMEN DE TESTS")
    print("="*80)

    passed = sum(1 for _, success in results if success)
    total = len(results)

    for name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status}: {name}")

    print(f"\nRESULTADO: {passed}/{total} tests pasaron")

    if passed == total:
        print("\nüéâ ¬°TODOS LOS TESTS PASARON! La integraci√≥n est√° completa.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è {total - passed} tests fallaron. Revisar implementaci√≥n.")
        return 1


if __name__ == "__main__":
    exit_code = run_all_tests()
    sys.exit(exit_code)
